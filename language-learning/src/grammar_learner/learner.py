# language-learning/src/learner.py                                      # 190410
import logging
import os, time  # pickle, numpy as np, pandas as pd
from copy import deepcopy
from shutil import copy2 as copy
from collections import OrderedDict, Counter
from .utl import UTC, kwa, sec2string
from .read_files import check_dir, check_mst_files
from .preprocessing import filter_links
from .pparser import files2links, lines2links, filter_lines
from .corpus_stats import corpus_stats
from .category_learner import learn_categories, cats2list
from .grammar_inducer import induce_grammar, add_disjuncts, check_cats
from .generalization import generalize_categories, generalize_rules, \
                            generalise_rules, add_upper_level
from .write_files import list2file, save_link_grammar, save_cat_tree
from ..common.cliutils import handle_path_string

__all__ = ['learn_grammar', 'learn']


def learn(**kwargs):
    logger = logging.getLogger(__name__ + ".learn")
    verbose = kwa('none', 'verbose', **kwargs)
    start = time.time()
    log = OrderedDict({'start': str(UTC()), 'learn_grammar': 'v.0.7.81231'})
    # input_parses = kwargs['input_parses']
    # output_grammar = kwargs['output_grammar']
    input_parses = handle_path_string(kwargs['input_parses'])
    output_grammar = handle_path_string(kwargs['output_grammar'])

    # WSD: word_sense disambiguation                                    # 190408
    wsd_symbol = kwa('', 'wsd_symbol', **kwargs)
    # prj_dir: project directory
    if os.path.isdir(output_grammar):
        prj_dir = output_grammar
    elif os.path.isfile(output_grammar):
        prj_dir = os.path.dirname(output_grammar)
    else:  # create prj_dir
        if check_dir(output_grammar, True, verbose):
            prj_dir = output_grammar
    log.update({'project_directory': prj_dir})

    output_categories = kwa('', 'output_categories', **kwargs)
    if output_categories == '':
        output_categories = prj_dir
    output_statistics = kwa('', 'output_statistics', **kwargs)
    if output_statistics != '':
        if os.path.isfile(output_statistics):
            corpus_stats_file = output_statistics
        elif os.path.isdir(output_statistics):
            corpus_stats_file = output_statistics + '/corpus_stats.txt'
        else: corpus_stats_file = prj_dir + '/corpus_stats.txt'
    else: corpus_stats_file = prj_dir + '/corpus_stats.txt'

    # Handle temporary directory setup
    temp_dir = kwa('', 'temp_dir', **kwargs)
    tmpath = kwa('', 'tmpath', **kwargs)  # legacy parameter support
    
    # Determine temporary directory path
    if temp_dir != '':
        # Use provided temp_dir if it exists
        if check_dir(temp_dir, False, verbose):
            tmpath = temp_dir
        else:
            # Fall back to default temp location
            logger.warning(f"Provided temp_dir '{temp_dir}' does not exist, using default")
            tmpath = os.path.abspath(os.path.join('..', 'tmp'))
    elif tmpath == '':
        # No temp directory specified, use default
        tmpath = os.path.abspath(os.path.join('..', 'tmp'))
    
    # Create temp directory if needed and update kwargs
    if check_dir(tmpath, True, verbose):
        kwargs['tmpath'] = tmpath
        temp_dir = tmpath
        logger.debug(f"Using temporary directory: {tmpath}")
    else:
        logger.error(f"Failed to create temporary directory: {tmpath}")
        raise OSError(f"Cannot create temporary directory: {tmpath}")

    parse_mode = kwa('lower', 'parse_mode', **kwargs)  # 'casefold' » default?
    context = kwa(2, 'context', **kwargs)
    clustering = kwa('kmeans', 'clustering', **kwargs)  # Clustering method selection
    cats_gen = kwa('off', 'categories_generalization', **kwargs)
    grammar_rules = kwa(2, 'grammar_rules', **kwargs)
    verbose = kwa('none', 'verbose', **kwargs)

    files, re01 = check_mst_files(input_parses, verbose)
    log.update(re01)
    if 'error' in re01:
        logger.error(f"Input file check failed: {re01}")
        log.update({'error': 'Input file validation failed', 'details': re01})
        return {'error': 'input_files'}, log
    kwargs['input_files'] = files

    '''Read parses, extract links to DataFrame (2018), + filter sentences'''

    if 'ull_parsing' in kwargs and kwargs['ull_parsing'] == "180829":
        links, re02 = files2links(**kwargs)
        # links: pd.DataFrame(columns=['word', 'link', 'count'])
    else:                                                               # 190417
        links, re02 = filter_links(files, **kwargs)
    log.update(re02)

    if len(links) < 1:  # Requested by @alexei-gl, issue #209           # 190426
        raise ValueError("Empty filtered dataset with max_sentence_length = "
                         + str(kwa(99, 'max_sentence_length', **kwargs))
                         + ", max_unparsed_words = "
                         + str(kwa(0, 'max_unparsed_words', **kwargs)))

    if 'corpus_stats' in log:
        list2file(log['corpus_stats'], corpus_stats_file)
        log.update({'corpus_stats_file': corpus_stats_file})
    else:
        logger.error("Corpus statistics not generated")
        log.update({'error': 'Failed to generate corpus statistics'})
        return {'error': 'corpus_stats_generation_failed'}, log

    '''Learn word categories'''

    categories, re03 = learn_categories(links, **kwargs)
    log.update(re03)
    if 'corpus_stats' in log and 'cleaned_words' in re03:
        log['corpus_stats'].extend([
            ['Number of unique words after cleanup', re03['cleaned_words']],
            ['Number of unique features after cleanup', re03['clean_features']]])
        list2file(log['corpus_stats'], corpus_stats_file)

    '''Generalize word categories'''  # Note: add_upper_level functionality integrated below
    '''
    if cats_gen == 'jaccard' or (cats_gen == 'auto' and clustering == 'group'):
        categories, re04 = generalize_categories(categories, **kwargs)
        log.update(re04)
    elif cats_gen == 'cosine' or (cats_gen == 'auto' and clustering == 'kmeans'):
        log.update({'generalization':
                    'none: vector-similarity based - maybe some day...'})
    else:
        log.update({'generalization': 'none: ' + str(cats_gen)})
    #'''

    '''Learn grammar'''

    if grammar_rules != context:
        context = kwargs['context']
        kwargs['context'] = kwargs['grammar_rules']
        links, re06 = files2links(**kwargs)
        kwargs['context'] = context

    categories = add_disjuncts(categories, links, **kwargs)
    # Check if every category has disjuncts
    categories_without_disjuncts = [i for i, disjuncts in enumerate(categories['disjuncts']) 
                                   if i > 0 and (disjuncts is None or len(disjuncts) == 0)]
    if categories_without_disjuncts:
        logger.warning(f"Categories without disjuncts: {categories_without_disjuncts}")
    #  ? categories = prune_cats(categories, **kwargs)  # [F] ⇒ induce_grammar?
    #  ? re = check_cats(categories, **kwargs)

    # "Fully connected rules": every cluster connected to all clusters
    if grammar_rules < 0:
        rules = deepcopy(categories)
        clusters = [i for i, x in enumerate(rules['cluster'])
                    if i > 0 and x is not None]
        rule_list = [tuple([-x]) for x in clusters] + \
                    [tuple([x]) for x in clusters]
        for cluster in clusters:
            rules['disjuncts'][cluster] = set(rule_list)
    else:
        rules, re07 = induce_grammar(categories, **kwargs)

    lengths = [len(x) for x in rules['disjuncts']]

    '''Generalize grammar rules'''

    if 'rules_generalization' in kwargs:
        if kwargs['rules_generalization'] in ['jaccard', 'legacy']:
            rules, re08 = generalize_rules(rules, **kwargs)
            log.update(re08)
        elif kwargs['rules_generalization'] in \
                ['hierarchical', 'fast', 'updated', 'new']:
            rules, re08 = generalise_rules(rules, **kwargs)
            log.update(re08)

    if '+' in verbose:
        log['rule_sizes'] = dict(Counter(
            [len(x) for i, x in enumerate(rules['words'])
             if rules['parent'][i] == 0]))

    '''Save word category tree, Link Grammar files: cat_tree.txt, dict...dict'''

    # 81126 3rd hierarchy level over rules:
    if 'top_level' in kwargs and kwargs['top_level'] > -1:
        tree, _ = add_upper_level(rules, **kwargs)
        re09 = save_cat_tree(tree, output_categories, verbose='none')
    else:
        re09 = save_cat_tree(rules, output_categories, verbose='none')
    
    # Check file save errors
    if 'cat_tree_file' in re09:
        cat_tree_file = re09['cat_tree_file']
        if not os.path.exists(cat_tree_file) or os.path.getsize(cat_tree_file) == 0:
            logger.error(f"Failed to save category tree file: {cat_tree_file}")
            re09['error'] = 'category_tree_save_failed'
        else:
            logger.info(f"Category tree saved successfully: {cat_tree_file}")
    else:
        logger.error("Category tree save operation failed - no file path returned")
        re09['error'] = 'category_tree_save_failed'
    
    log.update(re09)
    re10 = save_link_grammar(rules, output_grammar, grammar_rules)
    
    # Check grammar file save errors
    if 'grammar_file' in re10:
        grammar_file = re10['grammar_file']
        if not os.path.exists(grammar_file) or os.path.getsize(grammar_file) == 0:
            logger.error(f"Failed to save grammar file: {grammar_file}")
            re10['error'] = 'grammar_file_save_failed'
        else:
            logger.info(f"Grammar file saved successfully: {grammar_file}")
    else:
        logger.error("Grammar save operation failed - no file path returned")
        re10['error'] = 'grammar_file_save_failed'
    
    log.update(re10)
    log.update({'finish': str(UTC())})
    log.update({'grammar_learn_time': sec2string(time.time() - start)})

    return rules, log  # Return both rules and log for comprehensive analysis


def learn_grammar(**kwargs):  # Backwards compatibility with legacy calls
    rules, log = learn(**kwargs)
    return log


# Notes:

# 80802: poc05.py/category_learner ⇒ category_learner.py/learn_categories
# 80825: random clusters, interconnected ⇒ cleanup, commit 80828
# 81021 cleanup: Grammar Learner 0.6
# 81102 sparse wordspace agglomerative clustering
# 81126 def learn_grammar ⇒ def learn + decorator
# 81204-07 test and block (snooze) data pruning with max_disjuncts, etc...
# 81231 cleanup
# 190221 temp_dir, tmpath handling improved for Grammar Learner tutorial
# 190409 Optional WSD, kwargs['wsd_symbol']
# 190410 resolved empty filtered parses dataset issue
# 190426 raise ValueError in case of empty filtered dataset (requested by pipeline)
